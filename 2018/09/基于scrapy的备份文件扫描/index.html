<!DOCTYPE html>
<html lang="zh-CN">










<head>
    <meta charset="utf-8" />
    <link rel="apple-touch-icon" sizes="76x76" href="/favicon.ico">
    <link rel="icon" type="image/png" href="/favicon.ico">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0, shrink-to-fit=no" name="viewport" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="description" content="One way to choose one">
    <meta name="author" content="Misaki">
    <meta name="keywords" content="">
    <title>基于scrapy的备份文件扫描 ~ Misaki&#39;s Blog</title>
    <link rel="stylesheet" href="/css/Material_Icons.css">
    <!-- <link rel="stylesheet" href="/css/font-awesome.css"> -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css">
    <link rel="stylesheet" href="/css/main.css">
    
        <link rel="stylesheet" href="/css/post.css">
        
            <link rel="stylesheet" href="/css/Prettify/tomorrow-night-eighties.min.css">
        
    
</head>

<body class=" sidebar-collapse">
<nav class="navbar navbar-transparent navbar-color-on-scroll fixed-top navbar-expand-lg" color-on-scroll="100" id="sectionsNav">
    <div class="container">
        <div class="navbar-translate">
            <a class="navbar-brand" href="/">
                Misaki&#39;s Blog</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" aria-expanded="false" aria-label="Toggle navigation">
                <span class="sr-only">Toggle navigation</span>
                <span class="navbar-toggler-icon"></span>
                <span class="navbar-toggler-icon"></span>
                <span class="navbar-toggler-icon"></span>
            </button>
        </div>
            <div class="collapse navbar-collapse">
                <ul class="navbar-nav ml-auto">
                    
                        
                            <li class="nav-item">
                                <a class="nav-link" href="/archives/">
                                    archives
                                </a>
                            </li>
                        
                            <li class="nav-item">
                                <a class="nav-link" href="/about/">
                                    about
                                </a>
                            </li>
                        
                    
                    
                        
                            <li class="nav-item">
                                <a class="nav-link" rel="tooltip" title="" data-placement="bottom" href="https://github.com/MisakiKata" target="_blank" data-original-title="See me here">
                                    <i class="fa fa-github"></i>
                                </a>
                            </li>
                        
                            <li class="nav-item">
                                <a class="nav-link" rel="tooltip" title="" data-placement="bottom" href="https://www.t00ls.net/members-profile-12179.html" target="_blank" data-original-title="See me here">
                                    <i class="fa fa-twitch"></i>
                                </a>
                            </li>
                        
                    
                </ul>
            </div>
    </div>
</nav>
    
  <div class="page-header header-filter" data-parallax="true" style="background-image: url('/img/post-banner.jpg'); height: 70vh;">
    
      <div class="container">
        <h1 class="title text-center post_title">基于scrapy的备份文件扫描</h1>
        <p class="text-center"><b>Thursday, September 27th 2018, 10:51 am</b></p>
      </div>
    
  </div>

  
  
  
    <div class="row" style="margin: 0 0 0; z-index: 999;">
  <div class="col-md-8 offset-md-1">
    <div class="main main-raised">
      <div class="container">
        <div class="section">
          <div class="post_content">
              <p>Scrapy，Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动测试。</p>
<p>首先生成项目文件</p>
<pre><code>scrapy startproject spiderdata
</code></pre><p>生成成功后，会有以下目录结构，首先在spiderdata中的spider目录创建我们的spider文件。</p>
<p><img src="\2018\09\基于scrapy的备份文件扫描\1538017021874.png" alt="1538017021874"></p>
<p>备份文件扫描文件名，有两个选择，一是基于字典，二是根据url的备份文件名，从以往发现备份文件的结果上看，两种方式都是经常存在使用的。</p>
<p>于是创建backup文件，用来生成备份文件名，创建一个列表用来存储字典文件名，另外创建一个方法用来基于url生成备份文件名。</p>
<pre><code>#coding:utf-8

import urlparse

class backup(object):
    def __init__(self, url):
        self.url = url
        self.list2 = [&#39;/db.zip&#39;, &#39;/fdsa.rar&#39;, &#39;/ftp.rar&#39;, &#39;/gg.rar&#39;, &#39;/hdocs.rar&#39;, &#39;/hdocs.zip&#39;, &#39;/a.zip&#39;, &#39;/web.zip&#39;,
                      &#39;/web.rar&#39;, &#39;/1.rar&#39;, &#39;/bbs.rar&#39;, &#39;/www.root.rar&#39;,
                      &#39;/123.rar&#39;, &#39;/data.rar&#39;, &#39;/bak.rar&#39;, &#39;/oa.rar&#39;, &#39;/admin.rar&#39;, &#39;/www.rar&#39;, &#39;/2014.rar&#39;,
                      &#39;/2015.rar&#39;, &#39;/2016.rar&#39;, &#39;/2014.zip&#39;, &#39;/2015.zip&#39;, &#39;/2016.zip&#39;,
                      &#39;/2017.zip&#39;, &#39;/1.zip&#39;, &#39;/1.gz&#39;, &#39;/1.tar.gz&#39;, &#39;/2.zip&#39;, &#39;/2.rar&#39;, &#39;/123.rar&#39;, &#39;/123.zip&#39;, &#39;/a.rar&#39;,
                      &#39;/a.zip&#39;, &#39;/admin.rar&#39;, &#39;/back.rar&#39;, &#39;/backup.rar&#39;, &#39;/bak.rar&#39;,
                      &#39;/bbs.rar&#39;, &#39;/bbs.zip&#39;, &#39;/beifen.rar&#39;, &#39;/beifen.zip&#39;, &#39;/beian.rar&#39;, &#39;/data.rar&#39;, &#39;/data.zip&#39;,
                      &#39;/HYTop.rar&#39;, &#39;/root.rar&#39;, &#39;/Release.rar&#39;, &#39;/Release.zip&#39;, &#39;/sql.rar&#39;,
                      &#39;/test.rar&#39;, &#39;/template.rar&#39;, &#39;/template.zip&#39;, &#39;/upfile.rar&#39;, &#39;/vip.rar&#39;, &#39;/wangzhan.rar&#39;,
                      &#39;/wangzhan.zip&#39;, &#39;/web.rar&#39;, &#39;/web.zip&#39;, &#39;/website.rar&#39;, &#39;/www.rar&#39;,
                      &#39;/www.zip&#39;, &#39;/wwwroot.rar&#39;, &#39;/wwwroot.zip&#39;, &#39;/wz.rar&#39;]

    def backup(self):
        list_a = []
        parse = urlparse.urlparse(self.url)
        name = parse.netloc.split(&#39;.&#39;)
        name_url = parse.netloc.replace(&#39;.&#39;, &#39;&#39;)
        for i in [&#39;.rar&#39;, &#39;.zip&#39;, &#39;.tar.gz&#39;, &#39;.7z&#39;]:
            list_a.append(parse.scheme + &#39;://&#39; + parse.netloc + &#39;/&#39; + parse.netloc + i)   #http://www.baidu.com/www.baidu.com.zip
            if &#39;www&#39; in name:
                list_a.append(self.url + &#39;/&#39; + name[1] + i)   #http://www.baidu.com/baidu.zip
                list_a.append(self.url + &#39;/&#39; + &#39;&#39;.join(name[1:]) + i)     #http://www.baidu.com/baiducom.zip
            else:
                list_a.append(self.url + &#39;/&#39; + name[0] + i)       #http://www.baidu.com/baidu.zip
            list_a.append(self.url + &#39;/&#39; + name_url + i)     #http://www.baidu.com/wwwbaiducom.zip
        for x in self.list2:
            list_a.append(self.url + x)
        return list_a
</code></pre><p>在spider的爬虫文件中使用以下代码</p>
<pre><code>#coding:utf-8

import scrapy
from backup import backup
from ..items import SpiderdateItem

class spiderdata(scrapy.Spider):

    name = &quot;spiderdata&quot;
    content_type = [&#39;application/x-rar&#39;,&#39;application/x-gzip&#39;,&#39;application/zip&#39;,&#39;application/octet-stream&#39;,&#39;application/x-7z-compressed&#39;]

    def start_requests(self):
        with open(&#39;ip.txt&#39;,&#39;r&#39;) as f:
            for i in f.readlines():
                ip = i.strip(&#39;\n&#39;)
                back = backup(ip)
                url_ip = back.backup()
                for x in url_ip:
                    yield scrapy.Request(x, callback=self.parse,dont_filter=True)

    def parse(self, response):
        item = SpiderdateItem()
        if response.headers[&#39;Content-Type&#39;] in self.content_type:
            print &quot;[&quot; + str(response.status) + &quot;]&quot; + u&#39; 检测到存在备份文件的URL: &#39;+ response.url
            item[&#39;url&#39;] = response.url
            yield item
</code></pre><p>调用之前创建的备份文件名函数，使用start_requests来生成一个可迭代对象。</p>
<p>数据通过item来保存本地，所以在items中创建一个参数，并且在settings中开启item管道。</p>
<pre><code>url = scrapy.Field()
</code></pre><p>在piplines中创建本地文件保存文件，创建一次文件对象，写入后根据 爬虫关闭后再关闭本地文件。</p>
<pre><code>    def __init__(self):
        self.f = open(&quot;url.txt&quot;,&#39;w&#39;)

    def process_item(self, item, spider):
        self.f.write(item[&#39;url&#39;].encode(&quot;utf-8&quot;)+&#39;\n&#39;)
        return item

    def close_spider(self, spider):
        self.f.close()
</code></pre><p>因此只需在spiderdata中创建ip.txt文件即可，写入需要检测的url，另外如果不想看到scrapy的log输出，可以用在setting中添加如下：</p>
<pre><code>LOG_LEVEL = &#39;WARNING&#39;
</code></pre><p>只显示warning级的log输出.</p>

          </div>
          <br><br>
              
                <div class="license-wrapper">
                    <p>原文作者：<a href="">Misaki</a>
                    <p>原文链接：<a href="/2018/09/基于scrapy的备份文件扫描/">/2018/09/基于scrapy的备份文件扫描/</a>
                    <p>发表日期：<a href="/2018/09/基于scrapy的备份文件扫描/">September 27th 2018, 10:51:38 am</a>
                    <p>更新日期：<a href="/2018/09/基于scrapy的备份文件扫描/">September 27th 2018, 11:12:06 am</a>
                    <p>版权声明：本文采用<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可</p>
                </div>
              
          <br><br>
          <div>
              <p>
                       
                      <span class="badge badge-default">#&nbsp;python</span>
                      &nbsp;
                      
              </p>
          </div>
        </div>
      </div>  
    </div>
  </div>
  <!-- TOC -->
  
      <div class="">
        <div id="toc">
          <p class="toc-title"><i class="material-icons" style="vertical-align:middle">toc</i>Toc:</p> 
          <div id="tocbot"></div>
        </div>
      </div>
  
</div>


<!-- Comments -->
<div class="row">
    <div class="col-md-8 offset-md-2">
    
        
            <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    <script>               
        var disqus_shortname = '';
        var disqus_config = function () {
            this.page.url = '/2018/09/基于scrapy的备份文件扫描/'; 
            this.page.identifier = '/2018/09/基于scrapy的备份文件扫描/';
        };
        (function() { 
            var d = document, s = d.createElement('script');
            s.type = 'text/javascript';
            s.src = '//'+disqus_shortname+'.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>                                
</div>
        
    
    </div>
</div>
  

<footer class="footer footer-default">
        <div class="container">
          <div class="float-left" style="padding: 15px 0;">
              <b>不努力一下，怎么知道自己比别人差呢？</b>
          </div>
        </div>
</footer>
      <!--   Core JS Files   -->
      <script src="/js/core/jquery.min.js?v=3.2.1"></script>
      <script src="/js/main.js"></script>
      <script src="/js/core/popper.min.js"></script>
      <script src="/js/core/bootstrap-material-design.min.js"></script>
      <script src="/js/plugins/moment.min.js"></script>
      <!-- Control Center for Material Kit: parallax effects, scripts for the example pages etc -->
      <script src="/js/material-kit.min.js?v=2.0.5"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
        <script src="/js/post.js"></script>
        <script src="/js/plugins/prettify.js"></script>
        <script>
            $(document).ready(function(){
                $('pre').addClass('prettyprint linenums');
                prettyPrint();
            })
        </script>
      
</body>
</html>